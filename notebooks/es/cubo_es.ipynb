{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de Representaciones Latentes con Autoencoder y UMAP\n",
    "\n",
    "En este proyecto, entrenamos un autoencoder para aprender representaciones latentes de imágenes. Usamos DBSCAN para agrupar las latentes y UMAP para visualizarlas en un espacio de 2 dimensiones. También incluimos la proyección de nuevas imágenes al espacio generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías y setup inicial\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.cluster import DBSCAN\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import joblib  \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Usando dispositivo: {device}')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparación de datos\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Clase personalizada para cargar imágenes desde una carpeta.\n",
    "    - img_dir: Ruta de la carpeta con imágenes\n",
    "    - transform: Transformaciones a aplicar a las imágenes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')  \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar la imagen {self.image_files[idx]}: {e}\")\n",
    "            return torch.zeros(1, 64, 64)  \n",
    "\n",
    "#Ruta a la carpeta con el dataset que se va a usar\n",
    "img_dir = \"\"\n",
    "\n",
    "# Verificar si la carpeta existe\n",
    "if not os.path.exists(img_dir):\n",
    "    raise FileNotFoundError(f\"La carpeta especificada no existe: {img_dir}\")\n",
    "\n",
    "# Crear el dataset y el dataloader\n",
    "dataset = CustomImageDataset(img_dir=img_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el modelo Autoencoder\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Autoencoder con codificador y decodificador.\n",
    "    - latent_dim: Dimensión del espacio latente\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 8 * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (64, 8, 8)),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),  \n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuración del entrenamiento\n",
    "\n",
    "latent_dim = 128\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "model = Autoencoder(latent_dim=latent_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        img = data.to(device)\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * img.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sample = img[:8]\n",
    "            reconstructed = model(sample)\n",
    "        sample = sample.cpu().numpy()\n",
    "        reconstructed = reconstructed.cpu().numpy()\n",
    "        fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "        for i in range(8):\n",
    "            axes[0, i].imshow(sample[i].squeeze(), cmap='gray')  \n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].imshow(reconstructed[i].squeeze(), cmap='gray')  \n",
    "            axes[1, i].axis('off')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar el modelo y extraer las representaciones latentes\n",
    "\n",
    "torch.save(model.state_dict(), 'autoencoder.pth')\n",
    "print(\"Modelo guardado como 'autoencoder.pth'\")\n",
    "\n",
    "dataloader_no_shuffle = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=0)  \n",
    "model.eval()\n",
    "\n",
    "latent_representations = []\n",
    "image_filenames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in dataloader_no_shuffle:\n",
    "        img = data.to(device)\n",
    "        latent = model.encoder(img)\n",
    "        latent = latent.cpu().numpy()\n",
    "        latent_representations.append(latent)\n",
    "\n",
    "latent_representations = np.concatenate(latent_representations, axis=0)\n",
    "print(f'Tamaño de las representaciones latentes: {latent_representations.shape}')\n",
    "\n",
    "dbscan = DBSCAN(eps=5, min_samples=10)\n",
    "clusters = dbscan.fit_predict(latent_representations)\n",
    "print(f'Número de clusters encontrados: {len(set(clusters)) - (1 if -1 in clusters else 0)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualización con UMAP\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean')\n",
    "embedding = reducer.fit_transform(latent_representations)\n",
    "print(f'Tamaño del embedding UMAP: {embedding.shape}')\n",
    "\n",
    "umap_model_path = 'umap_model.joblib'\n",
    "umap_embedding_path = 'umap_embedding.npy'\n",
    "joblib.dump(reducer, umap_model_path)\n",
    "np.save(umap_embedding_path, embedding)\n",
    "print(f\"Modelo UMAP guardado como '{umap_model_path}'\")\n",
    "print(f\"Embedding UMAP guardado como '{umap_embedding_path}'\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=clusters, cmap='Spectral', s=5)\n",
    "plt.colorbar(scatter, label='Cluster ID')\n",
    "plt.title('Visualización de Clusters con UMAP')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para proyectar sobre el UMAP nuevas proyecciones\n",
    "\n",
    "def project_and_plot_new_image(new_image_path, autoencoder_path='autoencoder.pth',\n",
    "                               umap_model_path='umap_model.joblib',\n",
    "                               umap_embedding_path='umap_embedding.npy'):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torchvision import transforms\n",
    "    from PIL import Image\n",
    "    import joblib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    class Autoencoder(nn.Module):\n",
    "        def __init__(self, latent_dim=128):\n",
    "            super(Autoencoder, self).__init__()\n",
    "\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 16, 3, stride=2, padding=1),  \n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(64 * 8 * 8, latent_dim)\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(latent_dim, 64 * 8 * 8),\n",
    "                nn.ReLU(True),\n",
    "                nn.Unflatten(1, (64, 8, 8)),\n",
    "                nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),  \n",
    "                nn.Tanh()\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            latent = self.encoder(x)\n",
    "            return latent\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.Grayscale(),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  \n",
    "    ])\n",
    "\n",
    "    def load_autoencoder(model_path, latent_dim=128):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = Autoencoder(latent_dim=latent_dim).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "        return model, device\n",
    "\n",
    "    def load_umap_model(umap_model_path):\n",
    "        reducer = joblib.load(umap_model_path)\n",
    "        return reducer\n",
    "\n",
    "    def load_umap_embedding(umap_embedding_path):\n",
    "        embedding = np.load(umap_embedding_path)\n",
    "        return embedding\n",
    "\n",
    "    def process_image(image_path, transform):\n",
    "        image = Image.open(image_path).convert('RGB')  \n",
    "        image = transform(image)\n",
    "        image = image.unsqueeze(0)  \n",
    "        return image\n",
    "\n",
    "\n",
    "    def project_image(image_path, autoencoder, reducer, device):\n",
    "        image = process_image(image_path, transform).to(device)\n",
    "        with torch.no_grad():\n",
    "            latent = autoencoder.encoder(image)\n",
    "        latent = latent.cpu().numpy()\n",
    "\n",
    "        embedding = reducer.transform(latent)\n",
    "        return embedding\n",
    "\n",
    "    def visualize_projection(embedding, existing_embedding, image_path):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(existing_embedding[:, 0], existing_embedding[:, 1], c='lightgray', s=5, label='Datos Existentes')\n",
    "        plt.scatter(embedding[0, 0], embedding[0, 1], c='red', s=100, label='Nueva Imagen')\n",
    "        plt.legend()\n",
    "        plt.title('Proyección de Nueva Imagen en el Espacio UMAP')\n",
    "        plt.xlabel('UMAP 1')\n",
    "        plt.ylabel('UMAP 2')\n",
    "        plt.show()\n",
    "\n",
    "    if not os.path.exists(autoencoder_path):\n",
    "        raise FileNotFoundError(f\"No se encontró el archivo del autoencoder: {autoencoder_path}\")\n",
    "    if not os.path.exists(umap_model_path):\n",
    "        raise FileNotFoundError(f\"No se encontró el archivo del modelo UMAP: {umap_model_path}\")\n",
    "    if not os.path.exists(umap_embedding_path):\n",
    "        raise FileNotFoundError(f\"No se encontró el archivo del embedding UMAP: {umap_embedding_path}\")\n",
    "    if not os.path.exists(new_image_path):\n",
    "        raise FileNotFoundError(f\"No se encontró la imagen especificada: {new_image_path}\")\n",
    "\n",
    "    autoencoder, device = load_autoencoder(autoencoder_path, latent_dim=128)\n",
    "    reducer = load_umap_model(umap_model_path)\n",
    "    existing_embedding = load_umap_embedding(umap_embedding_path)\n",
    "\n",
    "    embedding = project_image(new_image_path, autoencoder, reducer, device)\n",
    "    print(f'Coordenadas UMAP de la nueva imagen: {embedding}')\n",
    "\n",
    "    visualize_projection(embedding, existing_embedding, new_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selección de la nueva imagen a proyectar en el UMAP\n",
    "\n",
    "nueva_imagen_path = '' \n",
    "\n",
    "project_and_plot_new_image(nueva_imagen_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
