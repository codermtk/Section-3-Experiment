{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of Latent Representations with Autoencoder and UMAP\n",
    "\n",
    "In this project, we train an autoencoder to learn latent representations of images. We use DBSCAN to cluster the latents and UMAP to visualise them in a 2-dimensional space. We also include the projection of new images to the generated space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries and initial setup\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.cluster import DBSCAN\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import joblib  \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "\n",
    "    \"\"\"    \n",
    "    Custom class to load images from a folder.\n",
    "    - img_dir: Path to the folder with images\n",
    "    - transform: Transformations to apply to the images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')  \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.image_files[idx]}: {e}\")\n",
    "            return torch.zeros(1, 64, 64)  \n",
    "\n",
    "#Path to the folder with the dataset to be used\n",
    "img_dir = \"\"\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(img_dir):\n",
    "    raise FileNotFoundError(f\"The specified folder does not exist: {img_dir}\")\n",
    "\n",
    "# Create the dataset and the dataloader\n",
    "dataset = CustomImageDataset(img_dir=img_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Autoencoder model\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Autoencoder with encoder and decoder.\n",
    "    - latent_dim: Latent space dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 8 * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (64, 8, 8)),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),  \n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training configuration\n",
    "\n",
    "latent_dim = 128\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "model = Autoencoder(latent_dim=latent_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        img = data.to(device)\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * img.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sample = img[:8]\n",
    "            reconstructed = model(sample)\n",
    "        sample = sample.cpu().numpy()\n",
    "        reconstructed = reconstructed.cpu().numpy()\n",
    "        fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "        for i in range(8):\n",
    "            axes[0, i].imshow(sample[i].squeeze(), cmap='gray')  \n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].imshow(reconstructed[i].squeeze(), cmap='gray')  \n",
    "            axes[1, i].axis('off')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model and extract latent representations\n",
    "\n",
    "torch.save(model.state_dict(), 'autoencoder.pth')\n",
    "print(\"Model saved as 'autoencoder.pth'\")\n",
    "\n",
    "dataloader_no_shuffle = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=0)  \n",
    "model.eval()\n",
    "\n",
    "latent_representations = []\n",
    "image_filenames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in dataloader_no_shuffle:\n",
    "        img = data.to(device)\n",
    "        latent = model.encoder(img)\n",
    "        latent = latent.cpu().numpy()\n",
    "        latent_representations.append(latent)\n",
    "\n",
    "latent_representations = np.concatenate(latent_representations, axis=0)\n",
    "print(f'Size of latent representations: {latent_representations.shape}')\n",
    "\n",
    "dbscan = DBSCAN(eps=5, min_samples=10)\n",
    "clusters = dbscan.fit_predict(latent_representations)\n",
    "print(f'Number of clusters found: {len(set(clusters)) - (1 if -1 in clusters else 0)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation with UMAP\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean')\n",
    "embedding = reducer.fit_transform(latent_representations)\n",
    "print(f'UMAP embedding size: {embedding.shape}')\n",
    "\n",
    "umap_model_path = 'umap_model.joblib'\n",
    "umap_embedding_path = 'umap_embedding.npy'\n",
    "joblib.dump(reducer, umap_model_path)\n",
    "np.save(umap_embedding_path, embedding)\n",
    "print(f\"UMAP model saved as '{umap_model_path}'\")\n",
    "print(f\"Embedding UMAP saved as '{umap_embedding_path}'\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=clusters, cmap='Spectral', s=5)\n",
    "plt.colorbar(scatter, label='Cluster ID')\n",
    "plt.title('Cluster visualisation with UMAP')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to project new projections on the UMAP\n",
    "\n",
    "def project_and_plot_new_image(new_image_path, autoencoder_path='autoencoder.pth',\n",
    "                               umap_model_path='umap_model.joblib',\n",
    "                               umap_embedding_path='umap_embedding.npy'):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torchvision import transforms\n",
    "    from PIL import Image\n",
    "    import joblib\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    class Autoencoder(nn.Module):\n",
    "        def __init__(self, latent_dim=128):\n",
    "            super(Autoencoder, self).__init__()\n",
    "\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(1, 16, 3, stride=2, padding=1),  \n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(64 * 8 * 8, latent_dim)\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(latent_dim, 64 * 8 * 8),\n",
    "                nn.ReLU(True),\n",
    "                nn.Unflatten(1, (64, 8, 8)),\n",
    "                nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU(True),\n",
    "                nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),  \n",
    "                nn.Tanh()\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            latent = self.encoder(x)\n",
    "            return latent\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.Grayscale(),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  \n",
    "    ])\n",
    "\n",
    "    def load_autoencoder(model_path, latent_dim=128):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = Autoencoder(latent_dim=latent_dim).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "        return model, device\n",
    "\n",
    "    def load_umap_model(umap_model_path):\n",
    "        reducer = joblib.load(umap_model_path)\n",
    "        return reducer\n",
    "\n",
    "    def load_umap_embedding(umap_embedding_path):\n",
    "        embedding = np.load(umap_embedding_path)\n",
    "        return embedding\n",
    "\n",
    "    def process_image(image_path, transform):\n",
    "        image = Image.open(image_path).convert('RGB')  \n",
    "        image = transform(image)\n",
    "        image = image.unsqueeze(0)  \n",
    "        return image\n",
    "\n",
    "\n",
    "    def project_image(image_path, autoencoder, reducer, device):\n",
    "        image = process_image(image_path, transform).to(device)\n",
    "        with torch.no_grad():\n",
    "            latent = autoencoder.encoder(image)\n",
    "        latent = latent.cpu().numpy()\n",
    "\n",
    "        embedding = reducer.transform(latent)\n",
    "        return embedding\n",
    "\n",
    "    def visualize_projection(embedding, existing_embedding, image_path):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(existing_embedding[:, 0], existing_embedding[:, 1], c='lightgray', s=5, label='Existing Data')\n",
    "        plt.scatter(embedding[0, 0], embedding[0, 1], c='red', s=100, label='New Image')\n",
    "        plt.legend()\n",
    "        plt.title('New Image Screening at the UMAP Space')\n",
    "        plt.xlabel('UMAP 1')\n",
    "        plt.ylabel('UMAP 2')\n",
    "        plt.show()\n",
    "\n",
    "    if not os.path.exists(autoencoder_path):\n",
    "        raise FileNotFoundError(f\"Autoencoder file not found: {autoencoder_path}\")\n",
    "    if not os.path.exists(umap_model_path):\n",
    "        raise FileNotFoundError(f\"UMAP model file not found: {umap_model_path}\")\n",
    "    if not os.path.exists(umap_embedding_path):\n",
    "        raise FileNotFoundError(f\"UMAP embedding file not found: {umap_embedding_path}\")\n",
    "    if not os.path.exists(new_image_path):\n",
    "        raise FileNotFoundError(f\"The specified image was not found: {new_image_path}\")\n",
    "\n",
    "    autoencoder, device = load_autoencoder(autoencoder_path, latent_dim=128)\n",
    "    reducer = load_umap_model(umap_model_path)\n",
    "    existing_embedding = load_umap_embedding(umap_embedding_path)\n",
    "\n",
    "    embedding = project_image(new_image_path, autoencoder, reducer, device)\n",
    "    print(f'UMAP coordinates of the new image: {embedding}')\n",
    "\n",
    "    visualize_projection(embedding, existing_embedding, new_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selection of the new image to be projected in UMAP\n",
    "\n",
    "new_image_path = '' \n",
    "\n",
    "project_and_plot_new_image(new_image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencoder_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
